Debugging memory leaks¶
调试内存溢出¶

In Scrapy, objects such as Requests, Responses and Items have a finite lifetime: they are created, used for a while, and finally destroyed.
在Scrapy中，类似Requests, Response及Items的对象具有有限的生命周期: 他们被创建，使用，最后被销毁

From all those objects, the Request is probably the one with the longest lifetime, as it stays waiting in the Scheduler queue until it’s time to process it.
这些对象中，Request的生命周期应该是最长的，其会在调度队列(Scheduler queue)中一直等待，直到被处理

For more info see Architecture overview.
 更多内容请参考 架构概览 

As these Scrapy objects have a (rather long) lifetime, there is always the risk of accumulating them in memory without releasing them properly and thus causing what is known as a “memory leak”.
由于这些Scrapy对象拥有很长的生命，因此将这些对象存储在内存而没有正确释放的危险总是存在

To help debugging memory leaks, Scrapy provides a built-in mechanism for tracking objects references called trackref, and you can also use a third-party library called Guppy for more advanced memory debugging (see below for more info).
为了帮助调试内存泄露，Scrapy提供了跟踪对象引用的机制，叫做 trackref ， 或者您也可以使用第三方提供的更先进内存调试库 Guppy (更多内容请查看下面)

Both mechanisms must be used from the Telnet Console.
而这都必须在 Telnet终端 中使用

Common causes of memory leaks¶
内存泄露的常见原因¶

It happens quite often (sometimes by accident, sometimes on purpose) that the Scrapy developer passes objects referenced in Requests (for example, using the meta attribute or the request callback function) and that effectively bounds the lifetime of those referenced objects to the lifetime of the Request.
内存泄露经常是由于Scrapy开发者在Requests中(有意或无意)传递对象的引用(例如，使用 meta 属性或request回调函数)，使得该对象的生命周期与 Request的生命周期所绑定

This is, by far, the most common cause of memory leaks in Scrapy projects, and a quite difficult one to debug for newcomers.
这是目前为止最常见的内存泄露的原因， 同时对新手来说也是一个比较难调试的问题

In big projects, the spiders are typically written by different people and some of those spiders could be “leaking” and thus affecting the rest of the other (well-written) spiders when they get to run concurrently, which, in turn, affects the whole crawling process.
在大项目中，spider是由不同的人所编写的

The leak could also come from a custom middleware, pipeline or extension that you have written, if you are not releasing the (previously allocated) resources properly.
与此同时，在不限制框架的功能的同时避免造成这些造成泄露的原因是十分困难的

For example, allocating resources on spider_opened but not releasing them on spider_closed may cause problems if you’re running multiple spiders per process.
因此， 我们决定不限制这些功能而是提供调试这些泄露的实用工具

By default Scrapy keeps the request queue in memory;
内存泄露可能存在与一个您编写的中间件，管道(pipeline) 或扩展，在代码中您没有正确释放 (之前分配的)资源

it includes Request objects and all objects referenced in Request attributes (e.g.
例如，您在 spider_opened 中分配资源但在 spider_closed 中没有释放它们

Debugging memory leaks with trackref¶
使用 trackref 调试内存泄露¶

trackref is a module provided by Scrapy to debug the most common cases of memory leaks.
trackref 是Scrapy提供用于调试大部分内存泄露情况的模块

It basically tracks the references to all live Requests, Responses, Item and Selector objects.
 简单来说，其追踪了所有活动(live)的Request, Request, Item及Selector对象的引用

You can enter the telnet console and inspect how many objects (of the classes mentioned above) are currently alive using the prefs() function which is an alias to the print_live_refs() function:
您可以进入telnet终端并通过 prefs() 功能来检查多少(上面所提到的)活跃(alive)对象

As you can see, that report also shows the “age” of the oldest object in each class.
正如所见，报告也展现了每个类中最老的对象的时间(age)

If you’re running multiple spiders per process chances are you can figure out which spider is leaking by looking at the oldest request or response.
如果您有内存泄露，那您能找到哪个spider正在泄露的机会是查看最老的request或response

You can get the oldest object of each class using the get_oldest() function (from the telnet console).
 您可以使用 get_oldest() 方法来获取每个类中最老的对象， 正如此所示(在终端中)(原文档没有样例)

Which objects are tracked?¶
哪些对象被追踪了

The objects tracked by trackrefs are all from these classes (and all its subclasses):
trackref 追踪的对象包括以下类(及其子类)的对象:

A real example¶
真实例子¶

Let’s see a concrete example of a hypothetical case of memory leaks.
让我们来看一个假设的具有内存泄露的准确例子

Suppose we have some spider with a line similar to this one:
假如我们有些spider的代码中有一行类似于这样的代码:

That line is passing a response reference inside a request which effectively ties the response lifetime to the requests’ one, and that would definitely cause memory leaks.
代码中在request中传递了一个response的引用，使得reponse的生命周期与request所绑定， 进而造成了内存泄露

Let’s see how we can discover the cause (without knowing it a-priori, of course) by using the trackref tool.
让我们来看看如何使用 trackref 工具来发现哪一个是有问题的spider(当然是在不知道任何的前提的情况下)

After the crawler is running for a few minutes and we notice its memory usage has grown a lot, we can enter its telnet console and check the live references:
当crawler运行了一小阵子后，我们发现内存占用增长了很多

The fact that there are so many live responses (and that they’re so old) is definitely suspicious, as responses should have a relatively short lifetime compared to Requests.
上面具有非常多的活跃(且运行时间很长)的response，而其比Request的时间还要长的现象肯定是有问题的

The number of responses is similar to the number of requests, so it looks like they are tied in a some way.
 因此，查看最老的response:

Sometimes extra information about live objects can be helpful.
就这样，通过查看最老的response的URL，我们发现其属于 somenastyspider.com spider

Let’s check the oldest response:
 现在我们可以查看该spider的代码并发现导致泄露的那行代码(在request中传递response的引用)

If you want to iterate over all objects, instead of getting the oldest one, you can use the scrapy.utils.trackref.iter_all() function:
如果您想要遍历所有而不是最老的对象，您可以使用 iter_all() 方法:

Too many spiders?¶
很多spider

If your project has too many spiders executed in parallel, the output of prefs() can be difficult to read.
如果您的项目有很多的spider，prefs() 的输出会变得很难阅读

For this reason, that function has a ignore argument which can be used to ignore a particular class (and all its subclases).
针对于此， 该方法具有 ignore 参数，用于忽略特定的类(及其子类)

For example, this won’t show any live references to spiders:
例如:将不会展现任何spider的活跃引用

scrapy.utils.trackref module¶
scrapy.utils.trackref模块¶

Here are the functions available in the trackref module.
以下是 trackref 模块中可用的方法

Inherit from this class (instead of object) if you want to track live instances with the trackref module.
如果您想通过 trackref 模块追踪活跃的实例，继承该类(而不是对象)

Print a report of live references, grouped by class name.
打印活跃引用的报告，以类名分类

Return the oldest object alive with the given class name, or None if none is found.
返回给定类名的最老活跃(alive)对象，如果没有则返回 None 

Use print_live_refs() first to get a list of all tracked live objects per class name.
首先使用 print_live_refs() 来获取每个类所跟踪的所有活跃(live)对象的列表

Return an iterator over all objects alive with the given class name, or None if none is found.
返回一个能给定类名的所有活跃对象的迭代器，如果没有则返回 None 

Use print_live_refs() first to get a list of all tracked live objects per class name.
首先使用 print_live_refs() 来获取每个类所跟踪的所有活跃(live)对象的列表

Debugging memory leaks with Guppy¶
使用Guppy调试内存泄露¶

trackref provides a very convenient mechanism for tracking down memory leaks, but it only keeps track of the objects that are more likely to cause memory leaks (Requests, Responses, Items, and Selectors).
trackref 提供了追踪内存泄露非常方便的机制，其仅仅追踪了比较可能导致内存泄露的对象 (Requests, Response, Items及Selectors)

However, there are other cases where the memory leaks could come from other (more or less obscure) objects.
然而，内存泄露也有可能来自其他(更为隐蔽的)对象

If this is your case, and you can’t find your leaks using trackref, you still have another resource: the Guppy library.
 如果是因为这个原因，通过 trackref 则无法找到泄露点，您仍然有其他工具: Guppy library 

If you use pip, you can install Guppy with the following command:
如果使用 setuptools , 您可以通过下列命令安装Guppy:

The telnet console also comes with a built-in shortcut (hpy) for accessing Guppy heap objects.
telnet终端也提供了快捷方式(hpy)来访问Guppy堆对象(heap objects)

Here’s an example to view all Python objects available in the heap using Guppy:
 下面给出了查看堆中所有可用的Python对象的例子:

You can see that most space is used by dicts.
您可以看到大部分的空间被字典所使用

Then, if you want to see from which attribute those dicts are referenced, you could do:
接着，如果您想要查看哪些属性引用了这些字典， 您可以:

As you can see, the Guppy module is very powerful but also requires some deep knowledge about Python internals.
如上所示，Guppy模块十分强大，不过也需要一些关于Python内部的知识

For more info about Guppy, refer to the Guppy documentation.
关于Guppy的更多内容请参考 Guppy documentation.

Leaks without leaks¶
Leaks without leaks¶

Sometimes, you may notice that the memory usage of your Scrapy process will only increase, but never decrease.
有时候，您可能会注意到Scrapy进程的内存占用只在增长，从不下降

Unfortunately, this could happen even though neither Scrapy nor your project are leaking memory.
不幸的是， 有时候这并不是Scrapy或者您的项目在泄露内存

This is due to a (not so well) known problem of Python, which may not return released memory to the operating system in some cases.
这是由于一个已知(但不有名)的Python问题

For more information on this issue see:
 Python在某些情况下可能不会返回已经释放的内存到操作系统

The improvements proposed by Evan Jones, which are detailed in this paper, got merged in Python 2.5, but this only reduces the problem, it doesn’t fix it completely.
改进方案由Evan Jones提出，在 这篇文章 中详细介绍，在Python 2.5中合并

To quote the paper:
 不过这仅仅减小了这个问题，并没有完全修复

To keep memory consumption reasonable you can split the job into several smaller jobs or enable persistent job queue and stop/start spider from time to time.
这个问题将会在未来Scrapy发布版本中得到解决

© Copyright 2008-2016, Scrapy developers.
© 版权所有 2008-2014, written by Scrapy developers, translated by Summer&Friends. Revision 5ed032cf.

