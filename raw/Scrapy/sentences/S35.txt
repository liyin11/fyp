Spider Middleware¶
Spider中间件(Middleware)¶

The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to process the requests and items that are generated from spiders.
下载器中间件是介入到Scrapy的spider处理机制的钩子框架，您可以添加代码来处理发送给 Spiders 的response及spider产生的item和request

Activating a spider middleware¶
激活spider中间件¶

To activate a spider middleware component, add it to the SPIDER_MIDDLEWARES setting, which is a dict whose keys are the middleware class path and their values are the middleware orders.
要启用spider中间件，您可以将其加入到 SPIDER_MIDDLEWARES 设置中

Here’s an example:
样例:

The SPIDER_MIDDLEWARES setting is merged with the SPIDER_MIDDLEWARES_BASE setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled middlewares: the first middleware is the one closer to the engine and the last is the one closer to the spider.
SPIDER_MIDDLEWARES 设置会与Scrapy定义的 SPIDER_MIDDLEWARES_BASE 设置合并(但不是覆盖)， 而后根据顺序(order)进行排序，最后得到启用中间件的有序列表: 第一个中间件是最靠近引擎的，最后一个中间件是最靠近spider的

To decide which order to assign to your middleware see the SPIDER_MIDDLEWARES_BASE setting and pick a value according to where you want to insert the middleware.
关于如何分配中间件的顺序请查看 SPIDER_MIDDLEWARES_BASE 设置，而后根据您想要放置中间件的位置选择一个值

The order does matter because each middleware performs a different action and your middleware could depend on some previous (or subsequent) middleware being applied.
 由于每个中间件执行不同的动作，您的中间件可能会依赖于之前(或者之后)执行的中间件，因此顺序是很重要的

If you want to disable a builtin middleware (the ones defined in SPIDER_MIDDLEWARES_BASE, and enabled by default) you must define it in your project SPIDER_MIDDLEWARES setting and assign None as its value.
如果您想禁止内置的(在 SPIDER_MIDDLEWARES_BASE 中设置并默认启用的)中间件， 您必须在项目的 SPIDER_MIDDLEWARES 设置中定义该中间件，并将其值赋为 None 

For example, if you want to disable the off-site middleware:
 例如，如果您想要关闭off-site中间件:

Finally, keep in mind that some middlewares may need to be enabled through a particular setting.
最后，请注意，有些中间件需要通过特定的设置来启用

See each middleware documentation for more info.
更多内容请查看相关中间件文档

Writing your own spider middleware¶
编写您自己的spider中间件¶

Each middleware component is a Python class that defines one or more of the following methods:
编写spider中间件十分简单

This method is called for each response that goes through the spider middleware and into the spider, for processing.
当response通过spider中间件时，该方法被调用，处理该response

process_spider_input() should return None or raise an exception.
process_spider_input() 应该返回 None 或者抛出一个异常

If it returns None, Scrapy will continue processing this response, executing all other middlewares until, finally, the response is handed to the spider for processing.
如果其返回 None ，Scrapy将会继续处理该response，调用所有其他的中间件直到spider处理该response

If it raises an exception, Scrapy won’t bother calling any other spider middleware process_spider_input() and will call the request errback.
如果其跑出一个异常(exception)，Scrapy将不会调用任何其他中间件的 process_spider_input() 方法，并调用request的errback

The output of the errback is chained back in the other direction for process_spider_output() to process it, or process_spider_exception() if it raised an exception.
 errback的输出将会以另一个方向被重新输入到中间件链中，使用 process_spider_output() 方法来处理，当其抛出异常时则带调用 process_spider_exception() 

This method is called with the results returned from the Spider, after it has processed the response.
当Spider处理response返回result时，该方法被调用

process_spider_output() must return an iterable of Request, dict or Item objects.
process_spider_output() 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)

This method is called when when a spider or process_spider_input() method (from other spider middleware) raises an exception.
当spider或(其他spider中间件的) process_spider_input() 跑出异常时， 该方法被调用

process_spider_exception() should return either None or an iterable of Response, dict or Item objects.
process_spider_exception() 必须要么返回 None ， 要么返回一个包含 Response 或 Item 对象的可迭代对象(iterable)

If it returns None, Scrapy will continue processing this exception, executing any other process_spider_exception() in the following middleware components, until no middleware components are left and the exception reaches the engine (where it’s logged and discarded).
如果其返回 None ，Scrapy将继续处理该异常，调用中间件链中的其他中间件的 process_spider_exception() 方法，直到所有中间件都被调用，该异常到达引擎(异常将被记录并被忽略)

If it returns an iterable the process_spider_output() pipeline kicks in, and no other process_spider_exception() will be called.
如果其返回一个可迭代对象，则中间件链的 process_spider_output() 方法被调用， 其他的 process_spider_exception() 将不会被调用

New in version 0.15.
0.15 新版功能.

This method is called with the start requests of the spider, and works similarly to the process_spider_output() method, except that it doesn’t have a response associated and must return only requests (not items).
该方法以spider 启动的request为参数被调用，执行的过程类似于 process_spider_output() ，只不过其没有相关联的response并且必须返回request(不是item)

It receives an iterable (in the start_requests parameter) and must return another iterable of Request objects.
其接受一个可迭代的对象(start_requests 参数)且必须返回另一个包含 Request 对象的可迭代对象

Note
注解

When implementing this method in your spider middleware, you should always return an iterable (that follows the input one) and not consume all start_requests iterator because it can be very large (or even unbounded) and cause a memory overflow.
当在您的spider中间件实现该方法时， 您必须返回一个可迭代对象(类似于参数start_requests)且不要遍历所有的 start_requests

The Scrapy engine is designed to pull start requests while it has capacity to process them, so the start requests iterator can be effectively endless where there is some other condition for stopping the spider (like a time limit or item/page count).
 该迭代器会很大(甚至是无限)，进而导致内存溢出

Built-in spider middleware reference¶
内置spider中间件参考手册¶

This page describes all spider middleware components that come with Scrapy.
本页面介绍了Scrapy自带的所有spider中间件

For information on how to use them and how to write your own spider middleware, see the spider middleware usage guide.
关于如何使用及编写您自己的中间件，请参考 spider middleware usage guide.

For a list of the components enabled by default (and their orders) see the SPIDER_MIDDLEWARES_BASE setting.
关于默认启用的中间件列表(及其顺序)请参考 SPIDER_MIDDLEWARES_BASE 设置

DepthMiddleware¶
DepthMiddleware¶

DepthMiddleware is a scrape middleware used for tracking the depth of each Request inside the site being scraped.
DepthMiddleware是一个用于追踪每个Request在被爬取的网站的深度的中间件

It can be used to limit the maximum depth to scrape or things like that.
 其可以用来限制爬取深度的最大深度或类似的事情

The DepthMiddleware can be configured through the following settings (see the settings documentation for more info):
DepthMiddleware 可以通过下列设置进行配置(更多内容请参考设置文档):

HttpErrorMiddleware¶
HttpErrorMiddleware¶

Filter out unsuccessful (erroneous) HTTP responses so that spiders don’t have to deal with them, which (most of the time) imposes an overhead, consumes more resources, and makes the spider logic more complex.
过滤出所有失败(错误)的HTTP response，因此spider不需要处理这些request

According to the HTTP standard, successful responses are those whose status codes are in the 200-300 range.
根据 HTTP标准 ，返回值为200-300之间的值为成功的resonse

If you still want to process response codes outside that range, you can specify which response codes the spider is able to handle using the handle_httpstatus_list spider attribute or HTTPERROR_ALLOWED_CODES setting.
如果您想处理在这个范围之外的response，您可以通过 spider的 handle_httpstatus_list 属性或 HTTPERROR_ALLOWED_CODES 设置来指定spider能处理的response返回值

For example, if you want your spider to handle 404 responses you can do this:
例如，如果您想要处理返回值为404的response您可以这么做:

The handle_httpstatus_list key of Request.meta can also be used to specify which response codes to allow on a per-request basis.
Request.meta 中的 handle_httpstatus_list 键也可以用来指定每个request所允许的response code

Keep in mind, however, that it’s usually a bad idea to handle non-200 responses, unless you really know what you’re doing.
不过请记住，除非您知道您在做什么，否则处理非200返回一般来说是个糟糕的决定

For more information see: HTTP Status Code Definitions.
更多内容请参考: HTTP Status Code定义.

HttpErrorMiddleware settings¶
HttpErrorMiddleware settings¶

Default: []
默认: []

Pass all responses with non-200 status codes contained in this list.
忽略该列表中所有非200状态码的response

Default: False
默认: False

Pass all responses, regardless of its status code.
忽略所有response，不管其状态值

OffsiteMiddleware¶
OffsiteMiddleware¶

Filters out Requests for URLs outside the domains covered by the spider.
过滤出所有URL不由该spider负责的Request

This middleware filters out every request whose host names aren’t in the spider’s allowed_domains attribute.
该中间件过滤出所有主机名不在spider属性 allowed_domains 的request

When your spider returns a request for a domain not belonging to those covered by the spider, this middleware will log a debug message similar to this one:
当spide返回一个主机名不属于该spider的request时， 该中间件将会做一个类似于下面的记录:

To avoid filling the log with too much noise, it will only print one of these messages for each new domain filtered.
为了避免记录太多无用信息，其将对每个新发现的网站记录一次

So, for example, if another request for www.othersite.com is filtered, no log message will be printed.
因此，例如， 如果过滤出另一个 www.othersite.com 请求，将不会有新的记录

But if a request for someothersite.com is filtered, a message will be printed (but only for the first request filtered).
 但如果过滤出 someothersite.com 请求，仍然会有记录信息(仅针对第一次)

If the spider doesn’t define an allowed_domains attribute, or the attribute is empty, the offsite middleware will allow all requests.
如果spider没有定义 allowed_domains 属性，或该属性为空， 则offsite 中间件将会允许所有request

If the request has the dont_filter attribute set, the offsite middleware will allow the request even if its domain is not listed in allowed domains.
如果request设置了 dont_filter 属性， 即使该request的网站不在允许列表里，则offsite中间件将会允许该request

RefererMiddleware¶
RefererMiddleware¶

Populates Request Referer header, based on the URL of the Response which generated it.
根据生成Request的Response的URL来设置Request Referer 字段

RefererMiddleware settings¶
RefererMiddleware settings¶

New in version 0.15.
0.15 新版功能.

Default: True
默认: True

Whether to enable referer middleware.
是否启用referer中间件

UrlLengthMiddleware¶
UrlLengthMiddleware¶

Filters out requests with URLs longer than URLLENGTH_LIMIT
过滤出URL长度比URLLENGTH_LIMIT的request

The UrlLengthMiddleware can be configured through the following settings (see the settings documentation for more info):
UrlLengthMiddleware 可以通过下列设置进行配置(更多内容请参考设置文档):

